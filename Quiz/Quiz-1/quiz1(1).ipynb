{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5dc987b",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ada798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a06d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/lamhoichun/Desktop/STAT3009/Lecture/train.csv')\n",
    "test_df = pd.read_csv('/Users/lamhoichun/Desktop/STAT3009/Lecture/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9d4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "test_df = test_df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "X_train = train_df[['user_id', 'item_id']].values\n",
    "y_train = train_df['rating'].values\n",
    "\n",
    "X_test = test_df[['user_id', 'item_id']].values\n",
    "y_test = test_df['rating'].values if 'rating' in test_df.columns else None\n",
    "\n",
    "n_user = max(train_df['user_id'].max(), test_df['user_id'].max()) + 1\n",
    "n_item = max(train_df['item_id'].max(), test_df['item_id'].max()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3d453",
   "metadata": {},
   "source": [
    "### User Average RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701cda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class for User Average RS\n",
    "class UserAvg(BaseEstimator):\n",
    "    def __init__(self, n_user, min_data=0):\n",
    "        self.n_user = n_user\n",
    "        self.min_data = min_data\n",
    "        self.global_avg = 0\n",
    "        self.user_avg = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_avg = np.mean(y)\n",
    "        self.user_avg = np.zeros(self.n_user)\n",
    "        for user_temp in range(self.n_user):\n",
    "            user_index = np.where(X[:,0] == user_temp)[0]\n",
    "            if len(user_index) <= self.min_data:\n",
    "                self.user_avg[user_temp] = self.global_avg\n",
    "            else:\n",
    "                self.user_avg[user_temp] = np.mean(y[user_index])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.user_avg[X[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d275b290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best UserAvg params: {'min_data': 10}\n",
      "Best UserAvg RMSE: 0.4758547556814469\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV for UserAvg\n",
    "user_model = UserAvg(n_user=n_user)\n",
    "param_grid_user = {'min_data': [0, 1, 2, 3, 5, 10]}\n",
    "\n",
    "grid_user = GridSearchCV(user_model, param_grid_user, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_user.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best UserAvg params:\", grid_user.best_params_)\n",
    "print(\"Best UserAvg RMSE:\", -grid_user.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d497be",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_user_model = grid_user.best_estimator_\n",
    "user_preds = best_user_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb377e5",
   "metadata": {},
   "source": [
    "### Item Average RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498038a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Item Average RS\n",
    "class ItemAvg(BaseEstimator):\n",
    "    def __init__(self, n_item, min_data=0):\n",
    "        self.n_item = n_item\n",
    "        self.min_data = min_data\n",
    "        self.global_avg = 0\n",
    "        self.item_avg = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_avg = np.mean(y)\n",
    "        self.item_avg = np.zeros(self.n_item)\n",
    "        for item_temp in range(self.n_item):\n",
    "            item_index = np.where(X[:,1] == item_temp)[0]\n",
    "            if len(item_index) <= self.min_data:\n",
    "                self.item_avg[item_temp] = self.global_avg\n",
    "            else:\n",
    "                self.item_avg[item_temp] = np.mean(y[item_index])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.item_avg[X[:,1]]\n",
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Define RMSE scorer\n",
    "# ===============================\n",
    "# rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eab8da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ItemAvg params: {'min_data': 0}\n",
      "Best ItemAvg RMSE: 0.12554968649627257\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV for ItemAvg\n",
    "item_model = ItemAvg(n_item=n_item)\n",
    "param_grid_item = {'min_data': [0, 1, 2, 3, 5, 10]}\n",
    "\n",
    "grid_item = GridSearchCV(item_model, param_grid_item, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_item.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best ItemAvg params:\", grid_item.best_params_)\n",
    "print(\"Best ItemAvg RMSE:\", -grid_item.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936c642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'min_data': 0}\n",
      "Best estimator: UserAvg(n_user=np.int64(199))\n"
     ]
    }
   ],
   "source": [
    "# Grid-Search for user_average model\n",
    "\n",
    "# Import the library for the GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "hp_grid = {'min_data': [0,1,2,3]}\n",
    "\n",
    "myuser = UserAvg(n_user=n_user)\n",
    "gs_user = GridSearchCV(estimator = myuser, \n",
    "                        param_grid = hp_grid, \n",
    "                        cv = 5, \n",
    "                        scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the model and make predictions\n",
    "gs_user.fit(X_train, y_train)\n",
    "y_pred = gs_user.predict(X_test)\n",
    "\n",
    "# Print the best hyper-parameters\n",
    "print(f\"Best parameters: {gs_user.best_params_}\")\n",
    "# Print the best estimator\n",
    "print(f\"Best estimator: {gs_user.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e632c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  user_pred  item_pred\n",
      "0      101       54   1.262194   1.581640\n",
      "1       18       81   1.516016   1.908355\n",
      "2       28       16   1.413403   0.954393\n",
      "3      152       63   1.421607   1.726933\n",
      "4       22       15   1.478470   0.893804\n"
     ]
    }
   ],
   "source": [
    "best_item_model = grid_item.best_estimator_\n",
    "item_preds = best_item_model.predict(X_test)\n",
    "\n",
    "# Save predictions\n",
    "test_df['user_pred'] = user_preds\n",
    "test_df['item_pred'] = item_preds\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c699798",
   "metadata": {},
   "source": [
    "### Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41129245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Hybrid Model\n",
    "class HybridAvg(BaseEstimator):\n",
    "    def __init__(self, n_user, n_item, min_data_user=0, min_data_item=0, alpha=0.5):\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.min_data_user = min_data_user\n",
    "        self.min_data_item = min_data_item\n",
    "        self.alpha = alpha\n",
    "        self.user_model = None\n",
    "        self.item_model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Train user model\n",
    "        self.user_model = UserAvg(n_user=self.n_user, min_data=self.min_data_user)\n",
    "        self.user_model.fit(X, y)\n",
    "\n",
    "        # Train item model\n",
    "        self.item_model = ItemAvg(n_item=self.n_item, min_data=self.min_data_item)\n",
    "        self.item_model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        user_preds = self.user_model.predict(X)\n",
    "        item_preds = self.item_model.predict(X)\n",
    "        return self.alpha * user_preds + (1 - self.alpha) * item_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d410c7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hybrid params: {'alpha': 0.0, 'min_data_item': 0, 'min_data_user': 0}\n",
      "Best Hybrid RMSE: 0.12554968649627257\n",
      "   user_id  item_id  user_pred  item_pred  hybrid_pred\n",
      "0      101       54   1.262194   1.581640     1.581640\n",
      "1       18       81   1.516016   1.908355     1.908355\n",
      "2       28       16   1.413403   0.954393     0.954393\n",
      "3      152       63   1.421607   1.726933     1.726933\n",
      "4       22       15   1.478470   0.893804     0.893804\n"
     ]
    }
   ],
   "source": [
    "param_grid_hybrid = {\n",
    "    'min_data_user': [0, 2, 5, 10],\n",
    "    'min_data_item': [0, 2, 5, 10],\n",
    "    'alpha': [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "hybrid_model = HybridAvg(n_user=n_user, n_item=n_item)\n",
    "\n",
    "grid_hybrid = GridSearchCV(hybrid_model, param_grid_hybrid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_hybrid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hybrid params:\", grid_hybrid.best_params_)\n",
    "print(\"Best Hybrid RMSE:\", -grid_hybrid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_hybrid = grid_hybrid.best_estimator_\n",
    "hybrid_preds = best_hybrid.predict(X_test)\n",
    "\n",
    "if y_test is not None:\n",
    "    print(\"Test RMSE Hybrid:\", np.sqrt(mean_squared_error(y_test, hybrid_preds)))\n",
    "\n",
    "test_df['hybrid_pred'] = hybrid_preds\n",
    "print(test_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
